# 活性化関数
ReLU 関数は
Play video starting at :5: and follow transcript5:00
、 今日のネットワーク設計で最も広く使用されている活性化関数であり、その主な利点は、 すべてのニューロンを同時に活性化しないことです。 softmax 関数は、
Play video starting at :5:11 and follow transcript5:11
各入力のクラスを定義する確率を取得しようとしていた分類器の出力層で使用するのが理想的です。 シグモイド関数と双曲線正接関数は、
Play video starting at :5:18 and follow transcript5:18
勾配が消失する問題につながる可能性があるため、多くの用途で使われていません。 モデルを構築するときは、 まず ReLU 関数を使用し、ReLU
Play video starting at :5:27 and follow transcript5:27
関数のパフォーマンスが良くない場合は、他のアクティベーション関数に切り替えることができます。（courasera IBM)
