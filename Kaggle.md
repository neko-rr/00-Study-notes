# 注意
- データの商用利用禁止
- コンペの期限:「Kaggle側でのコード自動処理の終了（＝スコア表示:Public Leaderboard）」を意味する。  
  ⇒期限前までにSubmitできたとしても、その後の自動処理中に期限を超えてしまうとエラーが出てしまい、無効になる。  
  　この処理に9時間以上かかってしまうと、時間超過エラーになってしまうので、Codeの修正が必要である。
- GPU/TPUはそれぞれ週約30時間しか使えないので貴重。  
  GPU/TPUを使わないコードの場合は、必ず使わない設定（右サイドバー内SettingsのAcceleratorをNone）にする。  
  デバッグしながら編集する場合、編集時はGPU/TPUを使わない設定にして、実行時だけGPU/TPUを使う設定に切り替える。  
  編集が終わり Codeを**Save & Run All(Commit)**した後は、必ずInteractive SessionをStop/Dismissにする。  
  ⇒編集中のCode（Interactive Session with GPU/TPU）とCommit中のCode（Version XX with GPU/TPU）がダブルカウントされてQuotaが減っていく。  
- KaggleではGPU環境が用意されており、1週間で30~40時間程度ですが無償利用が可能である。  
  利用できる時間の長さは週によって異なり、おそらく、Kaggle環境の空き状況や利用頻度等に応じて使える時間が割り振られる仕様になっていると思われる。  
  ***日本時間では毎週土曜日の朝9時にGPU使用時間がリセットされる。***
- タイムゾーンの記載はUTC（協定世界時）の場合が多いです．UTCから日本時間（JST）への変換には９時間を加える必要がある。
- １人１アカウントで，複数アカウントからのsubmit禁止。
- 「Private Sharing」は禁止されており，チーム外の者とソースコードなどを共有できない。
- submit回数の上限が設定されている（２～５回までのコンペが多い）。
- チームの最大人数が設定されている（最近は５人までのコンペが多い）。
- チームマージをするときは，全員のsubmit数の合計が「１日の最大submit数×コンペ開始からの日数」以内である必要がある。
# コンペ参加基準
- メダルあり  
This competitlon counts towards tiers.
- 扱いたいデータ(タグあり)
    - テーブル/tabular data
    - 画像/image data
    - テキスト/nlp,text data
    - その他(音声/sound,audio dataや動画/video)
- 開催期間
    - 通常2-3ヶ月
    - 初心者は、残り数週間-1ヶ月がオススメ
        - notebookやdiscussionが充実してる。
        - Vote数やコメントでどれが有力か分かる。

# Kaggleにおける実験管理の全体像
[目指せ決定！Kaggle実験管理術次第でコンペで成果を上げるためのノウハウ/p.9 図0.3:Kaggleにおける実験管理の全体像](https://github.com/neko-rr/00-Study-notes/blob/main/reading.md#%E7%9B%AE%E6%8C%87%E3%81%9B%E3%83%A1%E3%83%80%E3%83%AA%E3%82%B9%E3%83%88kaggle%E5%AE%9F%E9%A8%93%E7%AE%A1%E7%90%86%E8%A1%93-%E7%9D%80%E5%AE%9F%E3%81%AB%E3%82%B3%E3%83%B3%E3%83%9A%E3%81%A7%E6%88%90%E6%9E%9C%E3%82%92%E5%87%BA%E3%81%99%E3%81%9F%E3%82%81%E3%81%AE%E3%83%8E%E3%82%A6%E3%83%8F%E3%82%A6)
- コンペ理解
    - コンペ概要理解
    - 評価指標
        - 【+αコンペ前】
        - Notionテンプレ
        - 過去の経験の蓄積
        - コードスニペット
        - パイプライン
- EDA
    - データ理解
    - ドメイン知識理解
- ベースライン
    - コード管理
    - アウトプット管理
    - パラメータ管理
    - シード管理
- 提出
    - CV/LB管理
- 改善
    - アイデア管理
    - TODO管理
    - 結果の分析
        - 【+αコンペ後】
        - 振り返り
        - Late Submisiion
        - コード整理

# 実験管理
実験した手法と精度の上下等を管理して、次の実験に活かすためにデータベース化する。
過去の実験に戻れるようにデータを複製して、フォルダ管理。
ハイパーパラメータの管理をして、コードを見なくても何を変更したか分かるようにする。

# コードの再現性
- シードの固定：ランダムシードを固定して結果の一致を保証
- パラメータの保存：すべてのハイパーパラメータや設定を記録
- 独立したコード管理

⇒3つ代表的な方法あり  
- Gitで管理（コミット忘れると終わりだから、やめとこう）
- 1実験1Notebook（Kaggleで一般的）
- 1実験1ディレクトリ（実験数が多い人・慣れてきた人向け）

# 実験結果の整理方法
- 実験IDとファイル名を一致させる
## 管理表の最低必要項目
- 実験ID
- CV(Cross Validation Score)
- LB(Leaderboard Score)
- モデル
- 派生元（コピー元の実験ID）
- 備考（仮説：何をしたかったか）

## 実験結果の可視化
- CV/LB散布図
- 外部ツール利用
    - WandBが一番人気：LightGBMなどの主要な汎用ライブラリに対応しているため、幅広い用途で使用される(2025年1月時点)
        - クラウドベースで、ログは自動的にクラウドに保存される。
        - どこからでもアクセス可能で、チームメンバーと共有できる。
        - プロジェクト管理ツールとしての側面も強い。
    - 僅差でTensorBoard:深層学習特化(2025年1月時点)
        - ローカル環境。
        - 素早く可視化。
        - 元々は、TensorFlowのためにGoogleが開発したが、現在は他のフレームワークでも使用可能。

## ハイパーパラメータの管理
人間が設定する値  
### CFG(Configurationの略)クラスを用いたハイパーパラメータの管理
【メリット】
- 全てのハイパーパラメータを1箇所にまとめて管理しやすくする。
- 1実験1Notebookの人向き

【デメリット】
- パラメータを階層的に構造化できないこと

CFGクラスに全てのハイパーパラメータを集約(リスト1.5)
```Python
class CFG:
    learning_rate = 0.001
    batch_size = 64
    num_epochs = 10
    model_type = 'resnet50'
```
CFGクラスからハイパーパラメータを取得する例(リスト1.6)
```Python
# モデルの定義
model = build_model(model_type = CFG.model_type)

# 学習ループ
for epock in range(CFG.num_epocks):
    for batch in data_loader(CFG.batch_size):
        # 学習ステップ
        optimizer.zero_grad()
        outputs = model(batch['inputs'])
        loss = loss_fn(outputs, batch['labels'])
        loss.backward()
        optimizer.step()
```
### YAMLファイルの利用
### argparseとYAMLの併用によるハイパーパラメータ管理

# 生成AI活用
## 要約・抽出・翻訳
### コンペ概要
```md
あなたは優秀なデータサイエンティスト兼Kagglerです。
入力はKaggleのコンペ概要ページです。
以下のフォーマットに従って、日本語で要約して下さい：

# 出力フォーマット
コンペ背景：
コンペ概要：
評価指標：
コンペ期間：
賞金：
制限事項：

# 入力
（ここにコピーした内容を貼り付ける）
```
### データセットの理解
```md
あなたは優秀なデータサイエンティスト兼Kagglerです。
入力はKaggleのデータセットページです。
以下のフォーマットに従って、日本語で要約して下さい：

# 出力フォーマット
データの概要：
各ファイルの詳細な説明：

# 入力
（ここにコピーした内容を貼り付ける）
```
### ベースラインの理解（他の人のNotebook)
```md
あなたは優秀なデータサイエンティスト兼Kagglerです。
入力はKaggleのベースラインNotebookです。
以下のフォーマットに従って、Notebookでの実施要項を日本語で要約して下さい：

# 出力フォーマット
使用するデータ:
前処理:
モデルの定義:
学習の設定:
その他:

# 入力
（ここにコピーした内容を貼り付ける）
```
- 追加質問も可能
- 「～初学者にも分かるように文章のみで説明してください。」
### Discussionの要約
```md
以下のDiscussionを日本語で要約して下さい。
その上で何がスコア向上にとって重要か、また、効果的ではなかった取り組みについてもまとめて下さい。

（ここにコピーした内容を貼り付ける）
```
⇒改善案のブレスト
```md
Discussionでの議論を受けて、以下のベースラインの改善案をブレストして下さい。

（ベースラインを貼り付ける）
```

# 効率化
## 環境構築
### Kaggle内
#### 【メリット】
- 一番手間が無く、提供データの使用も容易
#### 【デメリット】
- 週30～40時間しかGPUが使用できない。
  ⇒画像系では、死活問題。
### Google Colaboratory(通称：Colab)
#### 【メリット】
- GPU・TPUが使用可能。実験を豊富にしたいならば、ほぼ必須。
- 無料または安価で使用可能
- ブラウザベースで利用可能なJupyterノートブック環境
  ⇒環境的に似てるらしい
- 実行結果とともにコードを共有できる
#### 【デメリット】
- Kaggle環境と違い、提供データを移動させるのに工夫が必要
  ⇒データを一度自分のPCにダウンロードが必要。
    画像データ大きい・空き容量。。。オープンなデータセットならば、Githubでダウンロード不要？？  
#### 【注意点】
- Pythonコードの実行が停止してしまう
    - 12時間ルール : ノートブックを実行して12時間経過
    - 90分ルール : ノードブックの使用を止めて90分経過
- /content.という drive とは別の場所で colab が実行されている。
  その colab notebook のランタイム(セッション)がリセットされると content内に保存したデータやモデルはすべて消し去られてしまいます。  
  しかし、Notebookからdriveをマウントして、drive上のフォルダにデータを保存すれば、問題は解決します。  
#### 【Google Drive上のファイルにアクセスする方法】
- Authorization codeを入力する方法
```Python
from google.colab import drive 
drive.mount('/content/drive')
```
⇒表示されているURLをクリックすると、認証コード取得画面に移動
- Google Colabのフォルダ画面からGoogle Driveを利用する方法
#### 【小技】
##### 実行しているのがColaboratoryかKaggleなのか判別するコード
```Python
import sys

# Colaboratory環境ならTrue
'google.colab' in sys.modules
# Kaggle Notebook環境ならTrue
'kaggle_web_client' in sys.modules
```
例えば、INPUTの位置を動的に変えるようにして、ColaboratoryでもKaggleでも動くコードにすることができます。
```Python
import sys
from pathlib import Path

if 'google.colab' in sys.modules:  # colab環境
    INPUT = Path(‘/content/input/’)
elif 'kaggle_web_client' in sys.modules:  # kaggle環境
    INPUT = Path(‘../input/’)
```
##### 高速化
- データがKaggleのデータの場合は、MyDriveからのコピーよりも、Kaggle apiでデータをダウンロードした方がやや速い。
- MyDriveのファイルの読み書きは遅いので、なるべくcontent直下にファイルを移動して読み込んだ方が良い
##### コード（セル）終了の通知音を鳴らす方法
```Python
from google.colab import output
def notif():
  output.eval_js('new Audio("https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg").play()')
```
あとは、実行したいコードの最後に『notif()』と入力することで、通知音を鳴らすことが可能。
#### 【便利URL】  
[Colaboratoryで分析コンペをする時のテクニック集](https://www.currypurin.com/entry/2021/03/04/070000)
[Google ColaboratoryでKaggleや機械学習：ファイル読み込み・保存・ライブラリインストール方法](https://zenn.dev/junko_ai/articles/1883b07e971096)  
[Kaggle notebookとGoogle Colabのデータ連携](https://qiita.com/nekot0/items/80d903a32ee101b165b6)  
[Colab で Kaggle (N番煎じ)](https://zenn.dev/mst8823/articles/da505dcf45474f)  
### Jupiter Notebook
普段Codeを書くときにJupyter Notebookを使用しているならば、Code全体など複数のセルにまたがるコピペには不向き。
その場合は、File→Editor TypeでScriptに切り替え、Shiftを押しながら画面を右クリックすると「すべて選択」ができるので、全体をコピペ可能である。  
### 自分のPC内に仮想環境作成
#### 【メリット】
- 縛りが無い  
#### 【デメリット】
- 環境構築が全部できる人のみ可能
- 30万以上するような高性能なPCが必要
## 学習と推論のファイルを分ける
- Kaggleの無料GPUの節約になる
- 待機時間の節約になる
- 方法は？

# 点数アップ手法
## 最初にコピーするNotebookの選択方法
- 「Most Votes」で並び替えして、人気の高いもの
- 
## モデルの考え方
過去に似たコンペ2,3コンペ漁って1~10位までの解法に目を通しつつ、現コンペのディスカッションを全部追って効くものを試すと銀メダルは取れるという肌感覚  
経験者のTwitter[https://twitter.com/an_nindouph/status/1725512487376945478]

## シード固定
- 機械学習の実験には乱数が使用されるため、シードを固定しないと、同じコードを実行しても結果が異なる場合がある。
- 実験結果の再現性が保証されないため、アルゴリズムやモデルの正しい評価ができない。
### seed_everything関数
複数の乱数生成器のシードを一括で固定する。
```Python
def seed_everything(seed: int):
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
```
1. random.seed(seed)
    - Pythonの組み込みモジュールrandomのシードを固定する。
    - randomモジュールを使用した乱数生成が再現可能になる。
2. os.environ["PYTHONHASHSEED"] = str(seed)
    - 環境変数PYTHONHASHSEEDを設定する。
    - Pythonのハッシュ関数のシードを固定し、辞書やセットの順序が再現可能になる。
3. np.random.seed(seed)
    - NumPyの乱数生成器のシードを固定する。
    - NumPyを使用した乱数生成が再現可能になる。
### seed_torch関数
PyTorchを使用する場合は、さらにPyTorch固有の乱数生成器のシードを固定する。
```Python
def seed_torch(seed = 42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
```
この関数は、seed_everythingの機能に加えて以下を行います。
1. torch.manual_seed(seed)
    - PyTorchのCPU上の乱数生成器のシードを固定する。
    - モデルの重みの初期化などに影響。
2. torch.cuda.manual_seed(seed)
    - PyTorchのCPU上の乱数生成器のシードを固定する。
    - GPUを使用した計算での再現性を確保。
3. torch.backends.cudnn.daterministic = True
    - CuDNNの決定論的アルゴリズムを使用。
    - 一部の非決定論的な挙動を排除。

## アンサンブル
コンペ終盤に実施。
- 初心者でも点数を上げやすい。
- あえて異なる特性を持つモデルを混ぜることで多様性が生まれ点数アップする。
### シングルモデルでseed変更
- 完全、初心者向け。
- 本当にseed変更して、多数決するだけ。
### 複数モデルでアンサンブル

## スタッキング

## チームマージ
チームマージの効果は、モデルの多様性を獲得して、アンサンブルでのスコアの向上のため。
金メダル手前の者同士が組んでアンサンブルして金メダルに到達したり、金メダル圏内の人同士で組んで賞金圏内に入ることも良くあるらしい。
モデルの多様性ということだと、チームマージ期限の直前にチームマージするのが一番良いが、いつチームマージするかは難しい。

# 提出基準
- 最終提出できるのは、2モデルのみ。
- 機械学習モデルの予測結果をsubmit方法。
    - KaggleのNotebook経由
    - csvファイルを直接アップロード
    - Kaggle APIを利用
## 2モデルの選び方例
1. CVベスト（手元のテストデータに対する精度のベスト）  
   Public LB ベスト（Publicリーダーボード上での精度のベスト）
2. 「安全な」モデル
   比較的「危険な」モデル
- Kagglerの間で「Trust CV」という言葉がある。
  「Public LBのスコアよりも，自分で計算したCVスコアを信じよう」という意味。

# エラー
## 問題が発生したため、このページはクラッシュしました。:Notebook
問題が解決しない場合は、当社側の問題である可能性があります。

'Node' で 'insertBefore' を実行できませんでした: 新しいノードを挿入する前のノードがこのノードの子ではありません。
NotFoundError: 'Node' で 'insertBefore' を実行できませんでした: 新しいノードを挿入する前のノードは、このノードの子ではありません。
- キャッシュのクリア: ブラウザのキャッシュが影響している可能性があります。キャッシュをクリアしてから再度試してみてください。
- ***ブラウザの拡張機能を確認: 特に広告ブロッカーなどのブラウザの拡張機能が影響を与えている場合があります。シークレットモードで通常は拡張機能が無効になりますが、念のため拡張機能をすべて無効にして正常に動作するか確認してください。***  
  ***⇒該当。自動翻訳を切ると、使用できる(´;ω;｀)***
- 原因でページの読み込みが中断されることがあります。別のネットワークに接続して試してみてください。
- Kaggleに問い合わせる: Kaggleのサポートに問い合わせて、問題が他のユーザーに共通のものであるかどうか確認することもおすすめします。
- 時間を置く: もしKaggle側で一時的な問題が発生している場合、時間を置くことで解決する可能性があります。

# 後で整理する
　右上の「Copy and Edit」をクリックすると，編集画面に遷移します．この時点ですでに元のNotebookとは別物なので，自分の好き勝手に編集して問題ありません．



Kaggleの順位表は「Leaderboard」と呼ばれ，Public LeaderboardとPrivate Leaderboardの２つに分かれています．Leaderboardは「LB」とも略されます．

　コンペ開催期間中から確認できるPublic LBはテストデータの一部で計算したスコアのLeaderboardであり，最終的な順位とは無関係です．

　Private LBは，最終順位となるLeaderboardです．このスコアは，Public LBのスコアに使われるテストデータとは別のテストデータで計算され，コンペ終了後にのみ確認できます．

　Kaggleでは，Public LBのスコアを気にし過ぎず，Private LBのスコアに使われるデータのスコアが高いモデルを作る必要があるといえます．

　Public LBとPrivate LBに使われるテストデータの割合は，コンペによって異なります．図2.6のように表示されるコンペの場合には，テストデータの50％がPublic LBのスコア，残りの50％のデータがPrivate LBのスコアを算出するために使われます．
Kaggleでは，最終順位に反映されるsubmitを，「My Submissions」タブで表示される自分のsubmitの一覧から２つまで選択可能です．図2.7のように「Use for Final Score」にチェックをつけたsubmitのみが，最終順位の算出（Private LBのスコア）に使われます．

Kaggleで勝つデータ分析の技術がKaggle本と言われおすすめ。
テーブルデータ最強。

LightGBMが流行中:特徴量の標準化不要
他の勾配ブースティングのパッケージであるXGBoostやCatBoostを使うこともあります．モデルの多様性を得る目的で，コンペ終盤に使うことが多いです．
私も似たやり方が多いですね．XGBoostはLightGBM登場前はKaggleを席巻していたので，２，３年前の解法を勉強すると多く登場します．CatBoostも最近頻繁に更新されていて，かなり可能性を感じますね．時代の変遷は激しいです

最近のコンペでは，金メダルを狙うならNNもほぼ必須になりつつありますね[38]．NNについては，欠損値補完や特徴量の標準化が必要など，特徴量エンジニアリングから変わってくるので難しさがあります．

公式documentationの「Parameters Tuning」[54]に従って，手動で調整を進めていきましょう．ここには，いくつかの目的別に，ハイパーパラメータ調整のコツが記載されています．

　今回は，性能を高めるのが目的なので「For Better Accuracy」を参照して次の調整をします．
￼１つ目は「大きめのmax_binを使え」です．defaultの値は255なので，ここでは300にしてみます．
￼２つ目は「小さめのlearning_rateを使え」です．defaultの値は0.1なので，ここでは0.05にしてみます．
￼３つ目は「大きめのnum_leavesを使え」です．defaultの値は31なので，ここでは40にしてみます．



　LightGBMは学習を高速化するため各特徴量をいくつかのヒストグラムに変換しています．max_binは各特徴量の最大の分割数を意味し，大きめの値を設定することで，機械学習アルゴリズムの表現力が上がる可能性があります．
　learning_rateは学習率です．小さめの値を設定することで「丁寧に」対応関係を学習するようになり，精度向上につながり得ます．
　num_leavesは１つの決定木における分岐の末端の最大数です．大きめの値を設定することで，機械学習アルゴリズムの表現力が上がる可能性があります．
　表現力を高める弊害として，計算量が多くなることや，過学習に陥る可能性が増加する点には注意しましょう．ここでは３つのハイパーパラメータを一度に変更していますが，そ

本節で１つのトピックとして取り上げたハイパーパラメータ調整ですが，個人的にはKaggle文脈ではそこまで重要でもない気はしますね．特にテーブルデータコンペで勾配ブースティング系のモデルを使っている場合は，よほど的外れな設定でなければ，特徴量エンジニアリングなど他のことに注力したほうが期待値が高い印象です．

私は序盤と終盤，２回ハイパーパラメータ調整することが多いかもしれません．前者は最初のベンチマークを作る，後者は最後の一押しの目的です．学習用データセットと検証用データセットそれぞれに対する性能を見ながら，手動で調整しますね．


Public LBで良いスコアが出ても，一部のデータのみに過学習した結果の可能性がある
Public LBとPrivate LBの分割方法は参加者に開示されない場合が多いので，Public LBにどのようなデータが使われている分からないという問題も存在します．

学習用データセットから検証用データセットを作る

　以上の問題を踏まえてKaggleでは，学習用データセットから検証用データセットを作成し，自分のモデルの性能を測るのが一般的です．

　自分で学習用データセットから検証用データセットを切り出すため，検証用データセットは目的変数を含めて全容を把握できています．

　実際にsubmitしなくても手元でスコアを計算できるので，submit回数に制限があるという問題に対処できます．検証用データセットの作成方法次第ではありますが，全体像の見えていないPublic LBのスコアに比べて，信頼に足る存在になる可能性があります．
Cross Validation（交差検証）
　「Cross Validation（交差検証）」を実行すると，ホールドアウト検証の例よりも汎用的に性能を確認できます．Cross Validationとは，図2.45のように複数回にわたって異なる方法でデータセットを分割し，それぞれでホールドアウト検証を実行する方法です．そのスコアの平均を確認することで，１回のホールドアウト検証で生じる偏りに対する懸念を弱めることができます．
Cross Validationを実施したときは，各分割でのスコアの平均をスコアと見なすことが多いです．このスコアのことを「CVスコア」と呼び，省略して単に「CV」と言うこともあります．
　学習用データセットを分割した最小単位をそれぞれ「fold」と呼びます（図2.46）．各分割で学習に使われなかったfoldは「Out-of-fold（oof）」と表現されます．
　oof_trainという変数名は「train（学習用データセット）のoof」という意味です．各分割でのoofに対する予測値を格納しています．

　繰り返しになりますが，Kaggleの目的は未知のデータセットであるPrivate LBに対する性能を高めることです．そのため，優れた検証用データセットとは，Private LBに似ているデータセットとなります．

　Private LBにおける「y==1」の割合は誰にも正確には分からないですが，例えば既存のデータセットである学習用データセットと同様の割合だと仮定するという考え方があります．この考え方では，検証用データセットも，学習用データセットから「y==1」の割合を保ったまま分割するのが理想的です．

　「y==1」の割合が均等でない場合，「y==1」を重要視したり逆に軽視したりと，機械学習アルゴリズムの学習がうまくいかない傾向にあります．このような状況では適切に特徴を学習できず，未知のデータセットに対する性能が劣化してしまう可能性があります．KFoldを用いた場合にスコアが悪化した原因もここにあると考えられます．

　ちなみに2.5.2節でtrain_test_split()を利用した際には，stratifyという引数でy_trainを指定することで，割合を保ったままデータセットを２つに分割していました．

￼

　割合を保ったままCross Validationを実施するためにはsklearnのStratifiedKFold()が利用可能です．学習用・検証用データセット内の「y==1」の割合が可能な範囲で均一に保たれます．
分割の際に気をつけたいことは，目的変数の割合以外にも，以下のような点があります．



￼データセット内に時系列性がないか
￼データセット内にグループが存在しないか



アンサンブルとは，複数の機械学習モデルを組み合わせることで性能の高い予測値を獲得する手法です．
　アンサンブルはKaggleなどのコンペにおける最後の一押しとして，大きな成果を発揮する場合があります．近年は多くのチームで，取り組みの深さは違えど至極当然に用いられる手法になっています．
　単純に一番良いモデルを選んだ場合，モデル A を採用すれば80％の正解率を得られる状況です．しかし，ここでアンサンブルを使うと80％以上の正解率を出すモデルを得ることができます．

　Kaggle Ensembling Guide
　https://mlwave.com/kaggle-ensembling-guide/ (Accessed: 30 November 2019).
　今回使うアンサンブルは，非常に単純な「多数決」の技法です．y0, y1, ..., y9 のそれぞれで各モデルの予測を確認し，多数決で最終的な予測結果を導出します．

　例えば y0 について，モデル A とモデル C は１を，モデル B は０を予測しています．そのため，最終的な予測結果は１とします．

　同様に考えていくと，下記が最終的な予測結果です．
sub_lgbm_skはsub_lgbm_hoとの相関が0.883077，sub_rfとの相関が0.796033であると読みとれます．sub_lgbm_hoとsub_rfとの相関は0.731329で，この３つの予測値の間で最も相関が小さい結果となりました．

　アンサンブルの観点では多様性が大切なので，予測値の相関が小さいほうが望ましいです．相関の大きさに絶対的な基準はありませんが，アンサンブルの文脈では0.95以下ならば十分に相関が小さいと言えるでしょう．今回はいずれの予測値も相関が小さいので，アンサンブルによる性能向上が期待できそうです．

　ここでは先の例と同様に，多数決で予測値を決定します．３ファイルの予測値部分を合計し，合計が２以上の場合は全体としての予測値を１とします．(p133)
　「Kaggle Ensembling Guide」には今回体験したような「csvファイルによるアンサンブル」だけではなく，より高度な「Stacked Generalization（Stacking）」「Blending」といったさまざまな技法が紹介されています．より深くアンサンブルを勉強したい場合は，一読をお勧めします．日本語の場合は『Kaggleで勝つデータ分析の技術』[36]に詳細な解説が記載されています．
　慣れないうちはアンサンブルは難しく感じるかもしれませんが，seedを変えて複数の予想を作り単純平均するだけでも効果はあります．「Seed Averaging」と呼ばれる手軽に試せる手法なので，やってみましょう．

　ハイパーパラメータの調整で伸ばせる余地もあります．




こういった背景もあり，画像コンペでは特徴量エンジニアリングよりも，ニューラルネットワークの構造設計に注力する面が大きい印象です．「第９回：Kaggleの「画像コンペ」とは--取り組み方と面白さを読み解く」[75]では，論文を読みながら研究動向を追うなど，Kaggle Masterの矢野さんの画像コンペへの取り組み方がまとめられています．画像コンペの取り組み方については，Kaggle Masterのphalanxさんの発表資料[91]も参考になります．

　他にも，データセットのサイズの違いは大きいです．例えばTitanicの「train.csv」は61KBですが，画像コンペのデータセットは10GBを超えることも珍しくありません．機械学習アルゴリズムにも層の深いニューラルネットワーク（ディープラーニング）を利用する場合が多く，計算量が多いです．計算資源としてGPUが必須になる場合が多いでしょう．

　ここでは，PyTorchのチュートリアルの冒頭部分からソースコードを流用しました．チュートリアルの続きでは，GPUを用いてディープラーニングの一種「Convolutional Neural Network（CNN）」[92]を学習し，予測を実行しています．興味があれば，ぜひ取り組んでみてください．

Public LBとPrivate LBが均等に分割されており，CVスコアとPublic LBのスコアに相関がある場合は，CVスコアを見ながら改善を進め，submitを重ねていくと良いでしょう．Public LBとPrivate LBの分割に偏りが疑われる場合，Private LBの分布を予想して検証用データセットを作ることもあります．

　効率的に実験を進めるために，慣れてきたら次のようなことに気をつけると良いでしょう．



￼（データサイズが大きい場合には特に）特徴量を何度も作り直すことはせず，一度作った特徴量は保存しておき後から使えるようにする

￼複数回実行する処理はモジュール化する
￼実験の内容と結果をスプレッドシートなどに記録し，後から見返せるようにする
￼過去のコンペで使ったソースコードは整理しておく
