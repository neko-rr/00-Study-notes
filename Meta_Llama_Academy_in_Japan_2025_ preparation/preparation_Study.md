# ハッカソン対策用
# 事前準備
## 準備済

## 準備前
- 自己紹介・チーム入れてアピールの資格リストと未経験注意表示の印刷1枚
- 使いそうなAPIの登録
  - Together API
- Google ColabでもGitHubが使えるようにする(使う可能性あり??)
- Cursor指示用の空フォーマット整理
  - ⇒便利そうなの見つけましたが、ただ1個のMarkdownファイル指定して伝えてたときより難しそうです。。。
  - [Spec Kit](https://github.com/github/spec-kit)
- GitHubの教科書を読む
- [LangChainアカデミー](https://academy.langchain.com/)を見る
- Streamlitで応答確認用の簡易チャット作る
# アイデア案2個(今回、極力発案者にならないようにしてチームに入る(｀・ω・´)ｷﾘｯ)
- 日本的な工場VS営業や営業VS事務等対立と棲み分けを緩く交流可視化して、建設的な意見に繋げようマインドマップ
  - 議事録や日報からマインドマップ
  - マインドマップへ、匿名コメント投稿
  - マインドマップから自動で議題提起or類似成功例や失敗例提示⇒匿名意見で広がる繋がり&匿名1人1意見1票投票
    - 独裁的上司が意見潰すの防止。
    - 過激発言連投禁止。
    - 仕事の渡し手or受け手、管理職、実務者、担当外部署人員、完全な部外者部署、効率化専門家等の立場で投票に任意の重み付け版と単純投票比較して、｢誰｣のために業務をどのように変更するかを検討&説明資料とする
- エゴサーチ効率化
  - 登録ロゴ画像や製品画像からトレンド集計
  - キーワードや社名、製品名からトレンド集計
  - ポジティブ&ネガティブ(本当の投稿見るのは任意で精神防御)
# 推奨スキル（全て満たさなくても可）
- ​Webアプリ／API実装（Python/Javascript 等、目安2–3年）
- 生成AIの基礎（LoRA/SFT/評価のいずれか）
- 大規模言語モデル (LLM) の基礎（ファインチューニング、RAG、量子化）
# 推奨スキル関連HP
- [LoRA（ローラ）](https://ledge.ai/articles/LoRA)
  - LoRAとは”Low-Rank Adaptation”の略で、特に大規模な事前学習済みモデルのファインチューニングに関連する技術
  - 計算量を減らしてファインチューニングを行う技術
  - 元のモデルのパラメータを直接変更する代わりに、低ランクの行列を導入して、パラメータの変更を行うことができる。少ない計算量で元のモデルに修正を加えることが可能になった。
  - 事前学習されたモデルの重みを固定し、変換器アーキテクチャの各層に低ランクの分解行列を注入することで、下流のタスク用の訓練可能なパラメータの数を大幅に削減する。例えば、GPT-3 175Bモデルにおいて、LoRAは訓練可能なパラメータを10,000倍減らし、GPUメモリ要件を3倍削減することを実現している。
  - 事例
    - カスタマイズされたチャットボット
    - AI写真集・AIモデル
- SFT(Supervised Fine-Tuning（教師ありファインチューニング）の略) 
# 主催者推奨初心者講座
[Prompt Engineering with Llama 2 & 3]
(https://www.deeplearning.ai/short-courses/prompt-engineering-with-llama-2/)  
[Introducing Multimodal Llama 3.2]
(https://www.deeplearning.ai/short-courses/introducing-multimodal-llama-3-2/)  
[Building with Llama 4]
(https://www.deeplearning.ai/short-courses/building-with-llama-4/)
# 初心者講座抜粋
```Python
# import llama helper function
from utils import llama
```
```Python
# define the prompt
prompt = "Help me write a birthday card for my dear friend Andrew."
```
```Python
# pass prompt to the llama function, store output as 'response' then print
response = llama(prompt)
print(response)

# 同時比較
```Python
prompt = f"""
Given the context `context:`,
Also also given the query (the task): `query:`
and given the name of several models: `mode:<name of model>,
as well as the response generated by that model: `response:`

Provide an evaluation of each model's response:
- Does it answer the query accurately?
- Does it provide a contradictory response?
- Are there any other interesting characteristics of the model's output?

Then compare the models based on their evaluation \
and recommend the models that perform the best.

context: ```{context}```

model: llama-2-7b-chat
response: ```{response_7b_chat}```

model: llama-2-13b-chat
response: ```{response_13b_chat}```

model: llama-2-70b-chat
response: ``{response_70b_chat}```

model: llama-3-8b-chat
response: ```{response_llama3_8b_chat}```

model: llama-3-70b-chat
response: ``{response_llama3_70b_chat}``
"""
```
```Python
response_eval = llama(prompt, 
                      model="META-LLAMA/Llama-3-70B-CHAT-HF")

print(response_eval)
```
# Copilot出力比較表
| 機能   | Together AI | AWS Bedrock | Groq | OpenRouter |
| ---- | ----------- | ----------- | ---- | ---------- |
| LoRA | ✅           | ❌           | ❌    | ❌          |
| SFT  | ✅           | ✅           | ❌    | ❌          |
| RAG  | アプリ側        | アプリ側        | アプリ側 | アプリ側       |
| 量子化  | ✅（QLoRA）    | ❌           | ❌    | ❌          |

## ✅ **Llama APIモデル比較表**
| **プロバイダー**      | **モデル**                          | **速度（tokens/sec）**                                                                                                                 | **価格（$ / 1M tokens）**                                                                                                                        | **特徴**                              |
| --------------- | -------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------- |
| **Together AI** | Llama 4 Maverick 17B             | 約117（70B）～171（13B） [\[aws.amazon.com\]](https://aws.amazon.com/bedrock/llama/)                           | 約$0.27（MoEモデル） [\[artificial...nalysis.ai\]](https://artificialanalysis.ai/models/llama-3-3-instruct-70b/providers)         | ・低コスト<br>・OpenAI互換API<br>・オンプレ対応    |
| **Groq**        | Llama 3.1 70B / 8B               | 約250（70B）～800（8B） [\[groq.com\]](https://groq.com/blog/the-official-llama-api-accelerated-by-groq) | 入力$0.59 / 出力$0.79（70B） [\[groq.com\]](https://groq.com/blog/the-official-llama-api-accelerated-by-groq)      | ・世界最速クラス<br>・低レイテンシ<br>・リアルタイム処理に最適 |
| **AWS Bedrock** | Llama 3.3 70B / Llama 3.2 Vision | 公開ベンチマークなし（GPUベース） [\[aws.amazon.com\]](https://aws.amazon.com/bedrock/llama/)                           | 入力$0.11 / 出力$0.34（70B） [\[artificial...nalysis.ai\]](https://artificialanalysis.ai/models/llama-3-3-instruct-70b/providers) | ・AWS統合<br>・セキュリティ重視<br>・企業向け        |

***

### ✅ **モデルの特徴**

*   **Llama 4 Maverick（Together AI）**
    → 最新MoEモデル、低コストで高速、マルチモーダル対応。
*   **Llama 3.1 / 3.3（Groq）**
    → 超高速推論（70Bでも250 tokens/sec）、リアルタイムアプリに最適。
*   **Llama 3.2 Vision（AWS Bedrock）**
    → 画像＋テキスト対応、企業利用に強い。

***

## ✅ **Llamaモデルスペック比較**
| モデル                  | パラメータ数          | コンテキスト長      | マルチモーダル対応  | 言語対応            | 日本語性能             | 商用利用    | 特徴                        |
| -------------------- | --------------- | ------------ | ---------- | --------------- | ----------------- | ------- | ------------------------- |
| **Llama 3.1**        | 8B / 70B / 405B | 最大128K       | ❌（テキストのみ）  | 多言語（英語中心）       | 強化済み（Llama 2より向上） | ✅（条件付き） | 高精度、長文対応、コード性能強化          |
| **Llama 3.2 Vision** | 11B / 90B       | 128K         | ✅（画像＋テキスト） | 英語＋8言語（画像は英語中心） | 良好（90Bで高精度）       | ✅（条件付き） | 初のオープンソースマルチモーダル、画像理解・OCR |
| **Llama 3.3**        | 70B             | 128K         | ❌          | 多言語             | 日本語特化版「Swallow」あり | ✅       | 軽量化＋推論コスト削減               |
| **Llama 4 Scout**    | 17B（MoE）        | 最大10M（1000万） | ✅          | 英語＋12言語         | 高精度（GPT-4o級）      | ✅       | 超長文処理、シングルGPU対応、低コスト      |
| **Llama 4 Maverick** | 17B（128エキスパート）  | 約1M（100万）    | ✅          | 英語＋12言語         | 高精度               | ✅       | マルチモーダル最強、画像推論・動画処理対応     |
| **Llama 4 Behemoth** | 288B（総2兆）       | 未公開（推定10M以上） | ✅          | 英語＋多言語          | 最強クラス             | ✅       | Meta史上最大、教師モデル、未リリース      |

***

### ✅ **公式言語対応**

*   Llama 3系：英語＋主要8言語（日本語含む）
*   Llama 4系：英語＋12言語（日本語含む）、マルチモーダルは英語中心

### ✅ **日本語能力**

*   Llama 3.3で日本語特化版「Swallow」あり
*   Llama 4は多言語対応強化、GPT-4o級の性能報告あり

### ✅ **商用利用**

*   Llama 2以降は**条件付きで商用利用可能**（MAU制限など）
*   最新モデルもオープンソース＋ライセンス遵守で利用可能

### ✅ **速度**

*   API利用時（Groq）：Llama 3.1 70Bで約250 tokens/sec、8Bで800 tokens/sec
*   Together AI：Llama 4 Maverickで117 tokens/sec（70B級）
*   Llama 4 ScoutはMoE構造で効率的、シングルGPUでも動作可能

### ✅ **特徴まとめ**

*   **Llama 3系**：長文対応（128K）、コード性能強化、Visionモデルで画像処理
*   **Llama 4系**：超長文（最大10M）、マルチモーダルネイティブ、MoEで低コスト高性能

***
# LangChain
- 大規模言語モデルの機能拡張を効率的に実装するためのライブラリ
- LangChain: OpenAI社のChatGPTに限らず、GoogleのGemini・Meta社のLlama・Anthropic社のClaudeなど、大規模言語モデルを使い分ける（切り替える）ことが容易になります。
- Retrievalとは「言語モデルが学習していない事柄に関して、外部データを用いて、回答を生成するための機能」
- Chainsは「複数のプロンプト入力を実行する機能」
- Memoryとは「ChainsやAgentsの内部における状態保持をする機能」
- Agentsとは「言語モデルに渡されたツールを用いて、モデル自体が、次にどのようなアクションを取るかを決定・実行・観測・完了するまで繰り返す機能」
- Callbacksとは「大規模言語モデルのアプリケーションのロギング、モニタリング、非同期処理などを効率的に管理する機能」
## LangGraph
- 並列して高速化
- 信頼性
  - 長時間（数分）実行するほど、失敗した時の影響が大きくなる
    - 短く分けて実行(Chains）
    - チェックポイントを活用・失敗した所から実行
## 参考
- [AWS：LangChain とは?](https://aws.amazon.com/jp/what-is/langchain/)
- [LangChainの概要と使い方](https://zenn.dev/umi_mori/books/prompt-engineer/viewer/langchain_overview)
# メモ
## 最大トークン>入力トークン+出力トークン
- 入出力のどちらかが長い場合は、もう一方の量が減るので、注意
- max_tokens=で制限すると安全
```Python
# set max_tokens to stay within limit on input + output tokens
prompt = f"""
Give me a summary of the following text in 50 words:\n\n
{text}
"""
response = llama(prompt,
                max_tokens=123)
```

## マルチターン
- 応答の記憶を残して、次の会話に繋げる
- 次の会話の際に、「前回のプロンプト+前回の回答+今回の質問」を合わせて依頼する
```Python
prompt_1 = """
    What are fun activities I can do this weekend?
"""
response_1 = llama(prompt_1)
```
```Python
prompt_2 = """
Which of these would be good for my health?
"""
```
```Python
chat_prompt = f"""
<s>[INST] {prompt_1} [/INST]
{response_1}
</s>
<s>[INST] {prompt_2} [/INST]
"""
print(chat_prompt)
```
```Python
response_2 = llama(chat_prompt,
                 add_inst=False,
                 verbose=True)
```
```Python
print(response_2)
```

## 出力内容の調整
- モデルにより、学習パラメータ数が異なる。 
  - 大きいほど、推論が遅く、複雑な事に対処可能
### 役割（ロール）
```Python
role = """
Your role is a life coach \
who gives advice to people about living a good life.\
You attempt to provide unbiased advice.
You respond in the tone of an English pirate.
"""

prompt = f"""
{role}
How can I answer this question from my friend:
What is the meaning of life?
"""
response = llama(prompt)
print(response)
```
### llama_guard

### その他
ゼロショットや例とか、要約や段階的には、自然言語の時と変わらないみたい

# code llama
プログラミングコードを出力
togethercomputer/CodeLlama-7b
togethercomputer/CodeLlama-13b
togethercomputer/CodeLlama-34b
togethercomputer/CodeLlama-7b-Python
togethercomputer/CodeLlama-13b-Python
togethercomputer/CodeLlama-34b-Python
togethercomputer/CodeLlama-7b-Instruct
togethercomputer/CodeLlama-13b-Instruct
togethercomputer/CodeLlama-34b-Instruct

# Llama Guard
